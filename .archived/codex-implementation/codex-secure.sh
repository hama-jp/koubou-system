#!/usr/bin/env bash
# ğŸ­ å·¥æˆ¿ã‚·ã‚¹ãƒ†ãƒ  Codex CLI - GPUæœ€é©åŒ–ç‰ˆ (O3æ¨å¥¨è¨­å®š)
set -euo pipefail

KOUBOU_HOME="$(cd "$(dirname "$0")/.."; pwd)"
PROJECT_ROOT="$(cd "$KOUBOU_HOME/.."; pwd)"


# GPUæœ€é©åŒ–Ollamaç’°å¢ƒå¤‰æ•° (O3æ¨å¥¨)
export OLLAMA_NUM_GPU=-1
export OLLAMA_GPU_LAYERS=-1
export OLLAMA_FLASH_ATTENTION=1
export OLLAMA_DEBUG=0  # å¿…è¦æ™‚ã¯1ã«å¤‰æ›´

# OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªï¼ˆãƒãƒ¼ãƒªãƒ³ã‚°æ–¹å¼ï¼‰
if ! curl -sf http://localhost:11434/v1/models >/dev/null 2>&1; then
    echo "â–¶ Starting Ollama with full-GPU off-load..."
    nohup ollama serve >> "$KOUBOU_HOME/logs/ollama.log" 2>&1 &
    
    # OllamaãŒæº–å‚™å®Œäº†ã¾ã§å¾…æ©Ÿ
    echo "â³ Waiting for Ollama to be ready..."
    until curl -sf http://localhost:11434/v1/models >/dev/null 2>&1; do
        sleep 1
    done
    echo "âœ… Ollama is ready"
fi

echo "ğŸ­ Codex (OSS) â€” GPU mode active"
echo "ğŸ“ Project: $PROJECT_ROOT"
echo "ğŸ”¥ GPU: Full off-load enabled"
echo "ğŸ“ Mode: Full-auto execution"
echo ""

# O3æ¨å¥¨ã®æœ€é©åŒ–Codex CLIå®Ÿè¡Œ (TTYå•é¡Œå¯¾å¿œç‰ˆ)
# ç’°å¢ƒå¤‰æ•°ã§TTYã‚’å¼·åˆ¶
export CODEX_FORCE_TTY=1
unset CI

exec codex exec \
    --oss \
    -m gpt-oss:20b \
    --dangerously-bypass-approvals-and-sandbox \
    --cd "$PROJECT_ROOT" \
    --skip-git-repo-check \
    "$@"