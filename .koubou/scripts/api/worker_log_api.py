#!/usr/bin/env python3
"""
ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ­ã‚°API - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°é…ä¿¡
"""

import os
import sys
import json
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from flask import Flask, jsonify, request
from flask_cors import CORS
import threading
import time
import re

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¨­å®š
KOUBOU_HOME = os.environ.get('KOUBOU_HOME', '/home/hama/project/koubou-system/.koubou')
sys.path.insert(0, os.path.join(KOUBOU_HOME, 'scripts'))

from common.database import get_db_manager

app = Flask(__name__)
CORS(app)

# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
LOG_DIR = Path(f"{KOUBOU_HOME}/logs/workers")

# ãƒ­ã‚°ãƒãƒƒãƒ•ã‚¡ï¼ˆãƒ¡ãƒ¢ãƒªå†…ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
log_buffer = []
MAX_BUFFER_SIZE = 500  # æœ€å¤§500ã‚¨ãƒ³ãƒˆãƒªã‚’ä¿æŒ

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
db = get_db_manager(f"{KOUBOU_HOME}/db/koubou.db")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LogParser:
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›"""
    
    @staticmethod
    def parse_log_line(line: str, worker_id: str) -> Optional[Dict]:
        """ãƒ­ã‚°è¡Œã‚’è§£æ"""
        try:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡º
            timestamp_match = re.match(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
            if timestamp_match:
                timestamp = timestamp_match.group(1)
                message = line[len(timestamp):].strip()
            else:
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                message = line.strip()
            
            # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š
            log_type = 'info'
            if any(word in line.lower() for word in ['error', 'âŒ', 'failed', 'exception']):
                log_type = 'error'
            elif any(word in line.lower() for word in ['warning', 'âš ï¸', 'warn']):
                log_type = 'warning'
            elif any(word in line.lower() for word in ['success', 'âœ…', 'completed']):
                log_type = 'success'
            elif any(word in line.lower() for word in ['processing', 'ğŸ”„', 'generating']):
                log_type = 'processing'
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„: é•·ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯åˆ‡ã‚Šè©°ã‚
            if len(message) > 200:
                message = message[:197] + '...'
            
            return {
                'timestamp': timestamp,
                'worker': worker_id,
                'type': log_type,
                'message': message
            }
        except Exception as e:
            logger.error(f"Failed to parse log line: {e}")
            return None


class LogMonitor:
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›£è¦–ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ã‚’æ¤œå‡º"""
    
    def __init__(self):
        self.file_positions = {}  # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®èª­ã¿å–ã‚Šä½ç½®ã‚’è¨˜éŒ²
        self.running = True
        
    def monitor_worker_logs(self):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›£è¦–"""
        while self.running:
            try:
                # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
                for log_file in LOG_DIR.glob("*.log"):
                    worker_id = log_file.stem
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
                    current_size = log_file.stat().st_size
                    last_position = self.file_positions.get(str(log_file), 0)
                    
                    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
                    if current_size > last_position:
                        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                            f.seek(last_position)
                            new_lines = f.readlines()
                            
                            for line in new_lines[-10:]:  # æœ€æ–°10è¡Œã®ã¿å‡¦ç†ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ï¼‰
                                if line.strip():
                                    parsed = LogParser.parse_log_line(line, worker_id)
                                    if parsed:
                                        add_log_entry(parsed)
                            
                            self.file_positions[str(log_file)] = f.tell()
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒç¸®å°ã—ãŸå ´åˆï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ï¼‰
                    elif current_size < last_position:
                        self.file_positions[str(log_file)] = 0
                
            except Exception as e:
                logger.error(f"Log monitoring error: {e}")
            
            time.sleep(1)  # 1ç§’ã”ã¨ã«ãƒã‚§ãƒƒã‚¯


def add_log_entry(entry: Dict):
    """ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ """
    global log_buffer
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¿½åŠ 
    if 'timestamp' not in entry:
        entry['timestamp'] = datetime.now().isoformat()
    
    log_buffer.append(entry)
    
    # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºåˆ¶é™
    if len(log_buffer) > MAX_BUFFER_SIZE:
        log_buffer = log_buffer[-MAX_BUFFER_SIZE:]


@app.route('/api/logs/recent', methods=['GET'])
def get_recent_logs():
    """æœ€è¿‘ã®ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—"""
    try:
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
        worker_id = request.args.get('worker')
        log_type = request.args.get('type')
        limit = min(int(request.args.get('limit', 100)), 200)  # æœ€å¤§200ä»¶
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_logs = log_buffer
        
        if worker_id and worker_id != 'all':
            filtered_logs = [log for log in filtered_logs if log['worker'] == worker_id]
        
        if log_type:
            filtered_logs = [log for log in filtered_logs if log['type'] == log_type]
        
        # æœ€æ–°ã®ã‚‚ã®ã‹ã‚‰è¿”ã™
        return jsonify({
            'status': 'success',
            'logs': filtered_logs[-limit:],
            'total': len(filtered_logs)
        })
        
    except Exception as e:
        logger.error(f"Error getting recent logs: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/logs/stats', methods=['GET'])
def get_log_stats():
    """ãƒ­ã‚°çµ±è¨ˆã‚’å–å¾—"""
    try:
        stats = {
            'total_entries': len(log_buffer),
            'by_type': {},
            'by_worker': {},
            'message_rate': 0
        }
        
        # ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
        for log in log_buffer:
            log_type = log.get('type', 'info')
            stats['by_type'][log_type] = stats['by_type'].get(log_type, 0) + 1
            
            worker = log.get('worker', 'unknown')
            stats['by_worker'][worker] = stats['by_worker'].get(worker, 0) + 1
        
        # ç›´è¿‘1åˆ†ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ¬ãƒ¼ãƒˆè¨ˆç®—
        one_minute_ago = datetime.now().timestamp() - 60
        recent_logs = [log for log in log_buffer 
                      if log.get('timestamp', '') > datetime.fromtimestamp(one_minute_ago).isoformat()]
        stats['message_rate'] = len(recent_logs) / 60  # messages per second
        
        return jsonify({
            'status': 'success',
            'stats': stats
        })
        
    except Exception as e:
        logger.error(f"Error getting log stats: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/workers/status', methods=['GET'])
def get_workers_status():
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—"""
    try:
        with db.get_connection() as conn:
            cursor = conn.execute("""
                SELECT worker_id, location, status, performance_factor,
                       tasks_completed, tasks_failed, endpoint_url, current_task
                FROM workers
                WHERE datetime('now', '-120 seconds') <= last_heartbeat
                   OR status = 'idle'
                ORDER BY location, worker_id
            """)
            
            workers = []
            for row in cursor.fetchall():
                worker = {
                    'worker_id': row[0],
                    'location': row[1] or 'local',
                    'status': row[2] or 'offline',
                    'performance_factor': row[3] or 1.0,
                    'tasks_completed': row[4] or 0,
                    'tasks_failed': row[5] or 0,
                    'endpoint_url': row[6],
                    'current_task': row[7]
                }
                
                # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯å†…å®¹ã‚’å–å¾—
                if worker['current_task']:
                    task_cursor = conn.execute("""
                        SELECT content FROM task_master
                        WHERE task_id = ?
                    """, (worker['current_task'],))
                    task_row = task_cursor.fetchone()
                    if task_row:
                        try:
                            content = json.loads(task_row[0]) if task_row[0] else {}
                            worker['current_task_content'] = content.get('prompt', '')[:100]
                        except:
                            worker['current_task_content'] = str(task_row[0])[:100]
                
                workers.append(worker)
        
        return jsonify({
            'status': 'success',
            'workers': workers
        })
        
    except Exception as e:
        logger.error(f"Error getting workers status: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/tasks/queue', methods=['GET'])
def get_task_queue():
    """ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ã®çŠ¶æ…‹ã‚’å–å¾—"""
    try:
        with db.get_connection() as conn:
            cursor = conn.execute("""
                SELECT task_id, priority, status, assigned_to, 
                       created_at, content
                FROM task_master
                WHERE status IN ('pending', 'in_progress')
                ORDER BY priority DESC, created_at ASC
                LIMIT 20
            """)
            
            tasks = []
            for row in cursor.fetchall():
                content_json = {}
                try:
                    content_json = json.loads(row[5]) if row[5] else {}
                except:
                    pass
                
                tasks.append({
                    'task_id': row[0],
                    'priority': row[1],
                    'status': row[2],
                    'assigned_to': row[3],
                    'created_at': row[4],
                    'type': content_json.get('type', 'general'),
                    'prompt_preview': content_json.get('prompt', '')[:50] + '...' if content_json.get('prompt', '') else ''
                })
            
            # çµ±è¨ˆæƒ…å ±ã‚‚è¿½åŠ 
            cursor = conn.execute("""
                SELECT 
                    COUNT(CASE WHEN status = 'pending' THEN 1 END) as pending_count,
                    COUNT(CASE WHEN status = 'in_progress' THEN 1 END) as in_progress_count,
                    COUNT(CASE WHEN status = 'completed' AND datetime('now', '-1 hour') <= updated_at THEN 1 END) as completed_last_hour
                FROM task_master
            """)
            stats = cursor.fetchone()
            
            return jsonify({
                'status': 'success',
                'tasks': tasks,
                'stats': {
                    'pending': stats[0] or 0,
                    'in_progress': stats[1] or 0,
                    'completed_last_hour': stats[2] or 0
                }
            })
            
    except Exception as e:
        logger.error(f"Error getting task queue: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/logs/tail/<worker_id>', methods=['GET'])
def tail_worker_log(worker_id):
    """ç‰¹å®šãƒ¯ãƒ¼ã‚«ãƒ¼ã®ãƒ­ã‚°ã‚’tailï¼ˆæœ€æ–°éƒ¨åˆ†ã‚’å–å¾—ï¼‰"""
    try:
        log_file = LOG_DIR / f"{worker_id}.log"
        
        if not log_file.exists():
            return jsonify({
                'status': 'error',
                'message': f'Log file not found for worker {worker_id}'
            }), 404
        
        # æœ€å¾Œã®50è¡Œã‚’å–å¾—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ï¼‰
        lines = []
        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()[-50:]
        
        # ãƒ‘ãƒ¼ã‚¹
        logs = []
        for line in lines:
            if line.strip():
                parsed = LogParser.parse_log_line(line, worker_id)
                if parsed:
                    logs.append(parsed)
        
        return jsonify({
            'status': 'success',
            'worker_id': worker_id,
            'logs': logs
        })
        
    except Exception as e:
        logger.error(f"Error tailing log for {worker_id}: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    # ãƒ­ã‚°ãƒ¢ãƒ‹ã‚¿ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
    monitor = LogMonitor()
    monitor_thread = threading.Thread(target=monitor.monitor_worker_logs, daemon=True)
    monitor_thread.start()
    
    # åˆæœŸãƒ­ã‚°
    add_log_entry({
        'worker': 'system',
        'type': 'success',
        'message': 'ğŸš€ Worker Log API started'
    })
    
    # APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    logger.info("Starting Worker Log API on port 8768")
    app.run(host='0.0.0.0', port=8768, debug=False)


if __name__ == '__main__':
    main()