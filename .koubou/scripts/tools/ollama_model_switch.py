#!/usr/bin/env python3
"""
Ollamaãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«åˆ‡ã‚Šæ›¿ãˆã‚‹
"""

import os
import sys
import argparse
import yaml
from pathlib import Path

# KOUBOUã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
KOUBOU_HOME = os.environ.get('KOUBOU_HOME', '/home/hama/project/koubou-system/.koubou')
sys.path.insert(0, os.path.join(KOUBOU_HOME, 'scripts'))

from common.ollama_config import get_ollama_config

def list_models():
    """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä¸€è¦§è¡¨ç¤º"""
    config = get_ollama_config()
    models = config.list_available_models()
    default_model = config.config.get('default_model')
    
    print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«:")
    print("=" * 60)
    
    for model_key in models:
        model_config = config.get_model_config(model_key)
        is_default = " â­ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)" if model_key == default_model else ""
        
        print(f"\nğŸ”¸ {model_key}{is_default}")
        print(f"   åå‰: {model_config.get('name')}")
        print(f"   èª¬æ˜: {model_config.get('description', 'N/A')}")
        
        use_cases = model_config.get('use_cases', [])
        if use_cases:
            print(f"   ç”¨é€”: {', '.join(use_cases[:3])}")
        
        options = model_config.get('options', {})
        if options:
            print(f"   è¨­å®š: temp={options.get('temperature', 'N/A')}, ctx={options.get('num_ctx', 'N/A')}")
    
    print("\n" + "=" * 60)

def show_model_details(model_key):
    """ç‰¹å®šãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã‚’è¡¨ç¤º"""
    config = get_ollama_config()
    
    if model_key not in config.list_available_models():
        print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{model_key}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªã™ã‚‹ã«ã¯ --list ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        return
    
    model_config = config.get_model_config(model_key)
    
    print(f"\nğŸ” ãƒ¢ãƒ‡ãƒ«è©³ç´°: {model_key}")
    print("=" * 60)
    print(f"åå‰: {model_config.get('name')}")
    print(f"èª¬æ˜: {model_config.get('description', 'N/A')}")
    
    use_cases = model_config.get('use_cases', [])
    if use_cases:
        print("\nç”¨é€”:")
        for use_case in use_cases:
            print(f"  â€¢ {use_case}")
    
    options = model_config.get('options', {})
    if options:
        print("\nã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š:")
        for key, value in options.items():
            print(f"  â€¢ {key}: {value}")
    
    print("\n" + "=" * 60)

def set_default_model(model_key):
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´"""
    config_path = os.path.join(KOUBOU_HOME, 'config', 'ollama_models.yaml')
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open(config_path, 'r', encoding='utf-8') as f:
        config_data = yaml.safe_load(f)
    
    if model_key not in config_data.get('models', {}):
        print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{model_key}' ãŒè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å­˜åœ¨ã—ã¾ã›ã‚“")
        return
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°
    old_default = config_data.get('default_model')
    config_data['default_model'] = model_key
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
    
    print(f"âœ… ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã—ã¾ã—ãŸ:")
    print(f"   {old_default} â†’ {model_key}")
    
    # ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã‚’è¡¨ç¤º
    config = get_ollama_config()
    config.reload_config()
    model_config = config.get_model_config(model_key)
    print(f"   åå‰: {model_config.get('name')}")
    print(f"   èª¬æ˜: {model_config.get('description', 'N/A')}")

def test_model(model_key=None):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    config = get_ollama_config()
    
    if model_key and model_key not in config.list_available_models():
        print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{model_key}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    model_name = config.get_model_name(model_key)
    model_options = config.get_model_options(model_key)
    server_host = config.get_server_host()
    
    print(f"\nğŸ§ª ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ: {model_key or 'default'}")
    print(f"   ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}")
    print(f"   ã‚µãƒ¼ãƒãƒ¼: {server_host}")
    
    # OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª
    import subprocess
    try:
        result = subprocess.run(
            ['curl', '-s', f'{server_host}/api/tags'],
            capture_output=True,
            timeout=5
        )
        if result.returncode != 0:
            print("âš ï¸  Ollamaã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
            print("   èµ·å‹•ã‚³ãƒãƒ³ãƒ‰: ollama serve")
            return
    except Exception as e:
        print(f"âš ï¸  Ollamaã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šã«å¤±æ•—: {e}")
        return
    
    # ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    try:
        result = subprocess.run(
            ['ollama', 'list'],
            capture_output=True,
            text=True,
            timeout=10
        )
        if model_name.split(':')[0] not in result.stdout:
            print(f"âš ï¸  ãƒ¢ãƒ‡ãƒ« {model_name} ãŒãƒ­ãƒ¼ã‚«ãƒ«ã«å­˜åœ¨ã—ã¾ã›ã‚“")
            print(f"   ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚³ãƒãƒ³ãƒ‰: ollama pull {model_name}")
            return
    except Exception as e:
        print(f"âš ï¸  ãƒ¢ãƒ‡ãƒ«ç¢ºèªã«å¤±æ•—: {e}")
        return
    
    print("âœ… ãƒ¢ãƒ‡ãƒ«ã¯åˆ©ç”¨å¯èƒ½ã§ã™")
    
    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®Ÿè¡Œ
    print("\nğŸ“ ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œä¸­...")
    try:
        # gemini-repo-cliã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
        gemini_repo_cli_path = os.path.join(KOUBOU_HOME, "..", "gemini-repo-cli", "src")
        if gemini_repo_cli_path not in sys.path:
            sys.path.insert(0, gemini_repo_cli_path)
        
        from gemini_repo.ollama_api import OllamaRepoAPI
        
        api = OllamaRepoAPI(model_name=model_name, host=server_host)
        if model_options:
            api.options.update(model_options)
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚¹ãƒˆ
        test_prompt = "Write a simple Python function that returns 'Hello, World!'"
        response = api.client.generate(
            model=model_name,
            prompt=test_prompt,
            stream=False,
            options=api.options
        )
        
        print("âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
        print("\n--- ç”Ÿæˆçµæœï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰---")
        print(response.get('response', '')[:200])
        print("...")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

def main():
    parser = argparse.ArgumentParser(
        description='Ollamaãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ„ãƒ¼ãƒ«',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§
  %(prog)s --list
  
  # ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°è¡¨ç¤º
  %(prog)s --show gpt-oss-20b
  
  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´
  %(prog)s --set-default qwen-coder-7b
  
  # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
  %(prog)s --test
  %(prog)s --test qwen-coder-1.5b
        """
    )
    
    parser.add_argument('--list', '-l', action='store_true',
                       help='åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º')
    parser.add_argument('--show', '-s', metavar='MODEL_KEY',
                       help='ç‰¹å®šãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã‚’è¡¨ç¤º')
    parser.add_argument('--set-default', '-d', metavar='MODEL_KEY',
                       help='ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´')
    parser.add_argument('--test', '-t', nargs='?', const='_default_', metavar='MODEL_KEY',
                       help='ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆMODEL_KEYçœç•¥æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰')
    
    args = parser.parse_args()
    
    # å¼•æ•°ãŒãªã„å ´åˆã¯ä¸€è¦§è¡¨ç¤º
    if not any([args.list, args.show, args.set_default, args.test]):
        list_models()
        print("\nğŸ’¡ ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º: python3 ollama_model_switch.py -h")
        return
    
    if args.list:
        list_models()
    
    if args.show:
        show_model_details(args.show)
    
    if args.set_default:
        set_default_model(args.set_default)
    
    if args.test is not None:
        model_key = None if args.test == '_default_' else args.test
        test_model(model_key)

if __name__ == "__main__":
    main()