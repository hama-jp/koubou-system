#!/usr/bin/env python3
"""
ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ - Gemini CLI (LMStudio) ã‚’ä½¿ç”¨ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†
"""

import json
import os
import subprocess
import sys
import logging
import time
import sqlite3
import signal
import atexit
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
KOUBOU_HOME = os.environ.get('KOUBOU_HOME', '/home/hama/project/koubou-system/.koubou')

# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
common_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..')
sys.path.insert(0, common_path)

try:
    from common.database import get_db_manager
    from common.python_executor import PythonExecutor
    from common.task_result_manager import TaskResultManager
    from common.ollama_config import get_ollama_config
    from common.config import get_config
except ImportError as e:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šçµ¶å¯¾ãƒ‘ã‚¹ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    koubou_scripts_path = os.path.join(KOUBOU_HOME, 'scripts')
    if koubou_scripts_path not in sys.path:
        sys.path.insert(0, koubou_scripts_path)
    from common.database import get_db_manager
    from common.python_executor import PythonExecutor
    from common.task_result_manager import TaskResultManager
    from common.ollama_config import get_ollama_config
    from common.config import get_config
DB_PATH = f"{KOUBOU_HOME}/db/koubou.db"
LOG_DIR = Path(f"{KOUBOU_HOME}/logs/workers")

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
db = get_db_manager(DB_PATH)

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
worker_id_for_log = os.environ.get('WORKER_ID', f"gemini_worker_{os.getpid()}")
LOG_DIR.mkdir(parents=True, exist_ok=True)
log_file = LOG_DIR / f"{worker_id_for_log}.log"

# loggingè¨­å®š
file_handler = logging.FileHandler(log_file)
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))

stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setLevel(logging.INFO)
stream_handler.setFormatter(logging.Formatter('%(message)s'))

logging.basicConfig(
    level=logging.INFO,
    handlers=[file_handler, stream_handler]
)

class GeminiLocalWorker:
    """Gemini Repo CLI (ollama + gpt-oss:20b) ã‚’ä½¿ç”¨ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼"""
    
    def __init__(self, model_key: Optional[str] = None):
        # Worker Pool ManagerçµŒç”±ã§ã®èµ·å‹•ã‚’ç¢ºèª
        auth_token = os.environ.get('WORKER_AUTH_TOKEN')
        expected_token = os.environ.get('KOUBOU_HOME', '') + '_POOL_MANAGER'
        
        if auth_token != expected_token:
            self.logger = logging.getLogger(__name__)
            self.logger.error("âŒ ãƒ¯ãƒ¼ã‚«ãƒ¼ã¯ Worker Pool Manager çµŒç”±ã§ã®ã¿èµ·å‹•å¯èƒ½ã§ã™")
            self.logger.error("ä½¿ç”¨æ–¹æ³•: .koubou/start_system.sh ã¾ãŸã¯ worker_pool_manager.py ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
            sys.exit(1)
        
        self.worker_id = os.environ.get('WORKER_ID', f"gemini_worker_{os.getpid()}")
        
        # Ollamaè¨­å®šã‚’èª­ã¿è¾¼ã¿
        self.ollama_config = get_ollama_config()
        
        # ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ¼ãŒç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å„ªå…ˆ
        model_key = model_key or os.environ.get('OLLAMA_MODEL_KEY', None)
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å–å¾—
        self.model_key = model_key
        self.model = self.ollama_config.get_model_name(model_key)
        self.model_options = self.ollama_config.get_model_options(model_key)
        self.server_host = self.ollama_config.get_server_host()
        
        # max_tokensã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆworkers.yamlã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ï¼‰
        self.max_tokens = int(os.environ.get('MAX_TOKENS', '32768'))
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Initializing Gemini worker {self.worker_id}")
        self.logger.info(f"Using model: {self.model} (key: {self.model_key or 'default'})")
        self.logger.info(f"Model options: {self.model_options}")
        self.logger.info(f"Max tokens: {self.max_tokens}")
        self.max_retries = 3
        self.timeout = 600  # 10åˆ†ã«å»¶é•·ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚«ã‚¦ãƒ³ãƒˆã‚¿ã‚¹ã‚¯å¯¾å¿œï¼‰
        
        # Pythonå®Ÿè¡Œç’°å¢ƒã®çµ±ä¸€åŒ–
        self.python_executor = PythonExecutor(KOUBOU_HOME)
        
        self.register_worker()
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ç™»éŒ²
        signal.signal(signal.SIGTERM, self.cleanup_handler)
        signal.signal(signal.SIGINT, self.cleanup_handler)
        atexit.register(self.cleanup)
    
    def register_worker(self):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²"""
        # æ—¢å­˜ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            with db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("DELETE FROM workers WHERE worker_id = ?", (self.worker_id,))
                conn.commit()
        except Exception as e:
            self.logger.warning(f"Failed to cleanup existing worker record: {e}")
        
        if db.register_worker(self.worker_id):
            self.logger.info(f"Gemini worker {self.worker_id} registered in DB.")
        else:
            self.logger.error(f"Failed to register worker {self.worker_id}")

    def get_next_task(self) -> Optional[Dict[str, Any]]:
        """æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—ï¼ˆã‚¢ãƒˆãƒŸãƒƒã‚¯ï¼‰"""
        try:
            # ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªã‚¿ã‚¹ã‚¯å–å¾—ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            row = db.acquire_next_task(self.worker_id)
            if row:
                task_id = row['task_id']
                self.logger.info(f"Picked up task {task_id}")
                
                content_str = row["content"] or '{}'
                context_str = '{}'
                
                try:
                    task = {
                        'task_id': task_id,
                        'content': json.loads(content_str),
                        'context': json.loads(context_str)
                    }
                    return task
                except json.JSONDecodeError as e:
                    self.logger.error(f"JSON decode error for task {task_id}: {e}. Content: {row.get('content', 'N/A')}")
                    self.fail_task(task_id, {"error": "Invalid JSON format in task content"})
                    return None
            else:
                return None
        except Exception as e:
            self.logger.error(f"Error getting next task: {e}", exc_info=True)
            return None

    def fail_task(self, task_id: str, result: Dict[str, Any]):
        """ã‚¿ã‚¹ã‚¯ã‚’å¤±æ•—ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        self.logger.warning(f"Marking task {task_id} as failed.")
        self.update_task_result(task_id, result)
    
    def check_for_task_notifications(self) -> Optional[str]:
        """Pool Managerã‹ã‚‰ã®ã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦é€šçŸ¥ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            with db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT task_id FROM worker_notifications
                    WHERE worker_id = ? AND notification_type = 'task_assigned' AND processed = 0
                    ORDER BY created_at ASC LIMIT 1
                """, (self.worker_id,))
                row = cursor.fetchone()
                
                if row:
                    task_id = row['task_id']
                    # é€šçŸ¥ã‚’å‡¦ç†æ¸ˆã¿ã«ãƒãƒ¼ã‚¯
                    cursor.execute("""
                        UPDATE worker_notifications 
                        SET processed = 1 
                        WHERE worker_id = ? AND task_id = ? AND notification_type = 'task_assigned'
                    """, (self.worker_id, task_id))
                    conn.commit()
                    
                    self.logger.info(f"ğŸ“¬ Received task assignment notification: {task_id}")
                    return task_id
                
        except Exception as e:
            self.logger.error(f"Error checking notifications: {e}")
            
        return None
    
    def get_assigned_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        """æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯IDã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—"""
        try:
            with db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT task_id, content 
                    FROM task_master
                    WHERE task_id = ? AND assigned_to = ? AND status = 'in_progress'
                """, (task_id, self.worker_id))
                row = cursor.fetchone()
                
                if row:
                    content_str = row["content"] or '{}'
                    
                    try:
                        task = {
                            'task_id': task_id,
                            'content': json.loads(content_str),
                            'context': {}
                        }
                        return task
                    except json.JSONDecodeError as e:
                        self.logger.error(f"Failed to decode task content: {e}")
                        return None
                    
        except Exception as e:
            self.logger.error(f"Error retrieving assigned task {task_id}: {e}")
            
        return None

    def process_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œå¯¾å¿œç‰ˆï¼‰"""
        task_content = task.get('content', {})
        task_type = task_content.get('type', 'general')
        prompt = task_content.get('prompt', '')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        input_files = task_content.get('files', [])
        output_file = task_content.get('output_file', None)
        
        # ãƒ‘ã‚¹æ¤œè¨¼ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ï¼‰
        config = get_config()
        for file_path in input_files:
            is_valid, error_msg = config.validate_file_operation(file_path)
            if not is_valid:
                self.logger.error(f"Invalid file path: {file_path} - {error_msg}")
                return {'success': False, 'output': '', 'error': f'Security validation failed: {error_msg}'}
        
        if output_file:
            is_valid, error_msg = config.validate_file_operation(output_file)
            if not is_valid:
                self.logger.error(f"Invalid output file path: {output_file} - {error_msg}")
                return {'success': False, 'output': '', 'error': f'Security validation failed: {error_msg}'}
        
        self.logger.info(f"Processing task {task['task_id']} of type '{task_type}' with Gemini Repo CLI")
        if input_files:
            self.logger.info(f"Input files: {input_files}")
        if output_file:
            self.logger.info(f"Output file: {output_file}")
        
        if not prompt:
            return {'success': False, 'output': '', 'error': 'Prompt is empty'}

        # LLMå‡¦ç†ä¸­ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«å¤‰æ›´
        db.update_worker_status(self.worker_id, 'processing', task['task_id'])

        # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œå¯¾å¿œç‰ˆã§Gemini CLIã‚’å®Ÿè¡Œ
        result = self.run_gemini_task_with_files(prompt, input_files, output_file)
        
        # ä½œæ¥­æˆæœç‰©ã‚’è‡ªå‹•ä¿å­˜ï¼ˆè¦ªæ–¹ç¢ºèªç”¨ï¼‰
        task_result = {
            'success': result.get('success', False),
            'output': result.get('output', ''),
            'error': result.get('error', None),
            'files_processed': len(input_files),
            'output_file': output_file,
            'prompt': prompt  # TaskResultManagerã§ä½¿ç”¨
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ç›´æ¥ä¿å­˜ã—ã¦è¦ªæ–¹ã®è² æ‹…è»½æ¸›
        self.logger.info(f"ğŸ”„ Starting automatic file save for task {task['task_id']}")
        try:
            self.save_deliverable_files(task['task_id'], task_result, prompt, task_type)
            self.logger.info(f"âœ… Automatic file save completed for task {task['task_id']}")
        except Exception as e:
            self.logger.error(f"âŒ Failed to save deliverable files for task {task['task_id']}: {e}")
            # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã‚‚å‡ºåŠ›
            import traceback
            self.logger.error(f"Stack trace: {traceback.format_exc()}")
        
        return task_result
    
    def run_gemini_task_with_files(self, prompt: str, input_files: list = None, output_file: str = None) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œå¯¾å¿œç‰ˆ Gemini Repo CLIã‚’ä½¿ç”¨ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç„¡åŠ¹åŒ–ï¼‰"""
        # å¸¸ã«gemini-repo-cliç›´æ¥å®Ÿè¡Œã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç„¡åŠ¹åŒ–ï¼‰
        return self.run_gemini_repo_cli_direct(prompt, input_files or [], output_file)
    
    def run_gemini_repo_cli_direct(self, prompt: str, input_files: list, output_file: str = None) -> Dict[str, Any]:
        """gemini-repo-cliã‚’ç›´æ¥å®Ÿè¡Œï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œæ©Ÿèƒ½ä»˜ãï¼‰"""
        import sys
        import os
        
        # gemini_repoãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
        project_root = os.path.join(KOUBOU_HOME, "..")
        gemini_repo_cli_path = os.path.join(project_root, "gemini-repo-cli", "src")
        if gemini_repo_cli_path not in sys.path:
            sys.path.insert(0, gemini_repo_cli_path)
            self.logger.info(f"Added gemini-repo-cli path: {gemini_repo_cli_path}")
        
        # virtualenvå†…ã®gemini_repoã‚‚è©¦ã™
        venv_path = f"{KOUBOU_HOME}/venv/lib/python3.11/site-packages"
        if venv_path not in sys.path:
            sys.path.insert(0, venv_path)
        
        try:
            # gemini_repoãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ç›´æ¥OllamaAPIã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆé…å»¶ãƒ­ãƒ¼ãƒ‰ç‰ˆï¼‰
            from gemini_repo.base_api import BaseRepoAPI
            from gemini_repo.ollama_api import OllamaRepoAPI
            
            # OllamaRepoAPIã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
            api = OllamaRepoAPI(model_name=self.model, host=self.server_host)
            
            # ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¨­å®š
            if hasattr(api, 'options'):
                # max_tokensã‚’num_ctxã¨ã—ã¦è¨­å®šï¼ˆOllamaã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åï¼‰
                api.options['num_ctx'] = self.max_tokens
                
                # ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚‚é©ç”¨
                if self.model_options:
                    api.options.update(self.model_options)
                
                self.logger.info(f"Applied model options: {api.options}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            project_root = os.path.join(KOUBOU_HOME, "..")
            repo_name = "koubou-system"
            target_file = output_file or "generated_content.txt"
            
            # ğŸ”§ çµ¶å¯¾ãƒ‘ã‚¹å¤‰æ›ï¼šç›¸å¯¾ãƒ‘ã‚¹ã‚’ project_root åŸºæº–ã®çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
            absolute_input_files = []
            for file_path in input_files:
                if os.path.isabs(file_path):
                    # æ—¢ã«çµ¶å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
                    absolute_input_files.append(file_path)
                else:
                    # ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯ project_root ã‚’åŸºæº–ã«çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
                    absolute_path = os.path.abspath(os.path.join(project_root, file_path))
                    absolute_input_files.append(absolute_path)
            
            self.logger.info(f"Using gemini-repo-cli with files: {input_files}")
            self.logger.info(f"Converted to absolute paths: {absolute_input_files}")
            
            # gemini-repo-cliã®generate_contentãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
            result = api.generate_content(
                repo_name=repo_name,
                file_paths=absolute_input_files,
                target_file_name=target_file,
                prompt=prompt
            )
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æ›¸ãè¾¼ã¿
            if output_file:
                output_path = os.path.join(project_root, output_file)
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(result)
                self.logger.info(f"Output written to: {output_file}")
            
            return {
                'success': True,
                'output': result,
                'error': None
            }
            
        except ImportError as e:
            self.logger.error(f"Failed to import gemini_repo modules: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç„¡åŠ¹åŒ–ï¼šImportErrorã§å¤±æ•—ã¨ã™ã‚‹
            return {
                'success': False,
                'output': '',
                'error': f'gemini-repo-cli import failed: {str(e)}'
            }
        except Exception as e:
            self.logger.error(f"Error in run_gemini_repo_cli_direct: {e}")
            return {
                'success': False,
                'output': '',
                'error': str(e)
            }
    
    def run_gemini_task(self, prompt: str) -> Dict[str, Any]:
        """æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³: æ–°ã—ã„run_gemini_task_with_filesã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„"""
        # å¸¸ã«ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œå¯¾å¿œç‰ˆã‚’ä½¿ç”¨ï¼ˆç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã§å‘¼ã³å‡ºã—ï¼‰
        return self.run_gemini_task_with_files(prompt, [], None)
    
    def run_gemini_task_legacy(self, prompt: str) -> Dict[str, Any]:
        """ãƒ¬ã‚¬ã‚·ãƒ¼ç‰ˆ: Gemini Repo CLIã‚’ã‚·ã‚§ãƒ«çµŒç”±ã§å®Ÿè¡Œï¼ˆéæ¨å¥¨ï¼‰"""
        gemini_script = f"{KOUBOU_HOME}/scripts/gemini-repo-exec.sh"
        
        for attempt in range(self.max_retries):
            try:
                self.logger.info(f"Executing Gemini Repo CLI script: {gemini_script} (attempt {attempt + 1}/{self.max_retries})")
                self.logger.debug(f"Prompt length: {len(prompt)} characters")
                self.logger.debug(f"Timeout set to: {self.timeout} seconds")
                
                # é•·æ™‚é–“ã‚¿ã‚¹ã‚¯ç”¨ã«ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚’é€ä¿¡ã—ãªãŒã‚‰å®Ÿè¡Œ
                import threading
                import signal
                
                # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé€ä¿¡ç”¨ã®ãƒ•ãƒ©ã‚°ã¨ã‚¹ãƒ¬ãƒƒãƒ‰
                self.processing = True
                def send_heartbeat():
                    heartbeat_count = 0
                    while self.processing:
                        heartbeat_count += 1
                        db.update_worker_status(self.worker_id, 'processing', None)
                        self.logger.debug(f"Heartbeat #{heartbeat_count} sent")
                        time.sleep(5)  # 5ç§’ã”ã¨ã«ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆ
                
                heartbeat_thread = threading.Thread(target=send_heartbeat, daemon=True)
                heartbeat_thread.start()
                
                # ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
                start_time = time.time()
                self.logger.info(f"Starting subprocess at {time.strftime('%Y-%m-%d %H:%M:%S')}")
                
                # ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã‚’èµ·å‹•
                process = subprocess.Popen(
                    [gemini_script, prompt],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    preexec_fn=os.setsid  # ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
                )
                
                self.logger.info(f"Subprocess started with PID: {process.pid}")
                
                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å¾…æ©Ÿ
                    stdout, stderr = process.communicate(timeout=self.timeout)
                    elapsed_time = time.time() - start_time
                    
                    self.logger.info(f"Subprocess completed in {elapsed_time:.2f} seconds")
                    self.logger.debug(f"Return code: {process.returncode}")
                    
                    # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé€ä¿¡ã‚’åœæ­¢
                    self.processing = False
                    heartbeat_thread.join(timeout=1)
                    
                    if process.returncode == 0:
                        self.logger.info(f"Task completed successfully")
                        return {
                            'success': True,
                            'output': stdout.strip(),
                            'error': None
                        }
                    elif process.returncode == 124:
                        # timeoutã‚³ãƒãƒ³ãƒ‰ã«ã‚ˆã‚‹ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        self.logger.error(f"Task killed by timeout command in script (exit code 124)")
                        if attempt < self.max_retries - 1:
                            wait_time = 2 ** attempt
                            self.logger.info(f"Retrying in {wait_time} seconds...")
                            time.sleep(wait_time)
                        else:
                            return {
                                'success': False,
                                'output': stdout,
                                'error': f'Script-level timeout (exit code 124): {stderr}'
                            }
                    else:
                        self.logger.error(f"Gemini Repo CLI script failed with code {process.returncode}")
                        self.logger.error(f"stderr: {stderr}")
                        if attempt < self.max_retries - 1:
                            wait_time = 2 ** attempt  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                            self.logger.info(f"Retrying in {wait_time} seconds...")
                            time.sleep(wait_time)
                        else:
                            return {
                                'success': False,
                                'output': stdout,
                                'error': stderr
                            }
                            
                except subprocess.TimeoutExpired:
                    elapsed_time = time.time() - start_time
                    self.logger.error(f"Python subprocess timeout after {elapsed_time:.2f} seconds")
                    
                    # ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—å…¨ä½“ã‚’çµ‚äº†
                    try:
                        os.killpg(os.getpgid(process.pid), signal.SIGTERM)
                        time.sleep(2)  # çµ‚äº†ã‚’å¾…ã¤
                        if process.poll() is None:
                            os.killpg(os.getpgid(process.pid), signal.SIGKILL)
                    except Exception as e:
                        self.logger.error(f"Failed to kill process group: {e}")
                    
                    # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé€ä¿¡ã‚’åœæ­¢
                    self.processing = False
                    
                    if attempt < self.max_retries - 1:
                        wait_time = 2 ** attempt
                        self.logger.info(f"Retrying in {wait_time} seconds...")
                        time.sleep(wait_time)
                    else:
                        return {'success': False, 'output': '', 'error': f'Python timeout after {elapsed_time:.2f}s'}
                        
            except Exception as e:
                # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé€ä¿¡ã‚’åœæ­¢
                self.processing = False
                self.logger.exception(f"Unexpected error in run_gemini_task: {e}")
                return {'success': False, 'output': '', 'error': str(e)}
        
        return {'success': False, 'output': '', 'error': 'Max retries exceeded'}

    def update_task_result(self, task_id: str, result: Dict[str, Any]):
        """ã‚¿ã‚¹ã‚¯çµæœã‚’æ›´æ–°ã—ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã«æˆ»ã™"""
        try:
            # æœ¬ç•ªç’°å¢ƒã§ã®æˆæœç‰©ä¿å­˜
            try:
                # ã‚¿ã‚¹ã‚¯å†…å®¹ã‚’å–å¾—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                task_content = result.get('prompt', f'Task {task_id}')
                
                manager = TaskResultManager()
                saved_files = manager.save_task_deliverable(
                    result, 
                    task_id, 
                    "general",  # task_type
                    task_content,  # task_content
                    5  # priority (default)
                )
                
                self.logger.info(f"ğŸ“‹ Task deliverable saved: {len(saved_files)} files")
                for file_type, file_path in saved_files.items():
                    self.logger.info(f"  {file_type}: {file_path.name}")
                    
            except Exception as e:
                self.logger.warning(f"Failed to save task deliverable: {e}")
            
            # ã‚¿ã‚¹ã‚¯å®Œäº†ã‚’DBã«è¨˜éŒ²ï¼ˆçµ±è¨ˆã‚‚åŒæ™‚ã«æ›´æ–°ï¼‰
            success = result.get('success', True)
            db.complete_task_with_stats(
                task_id,
                self.worker_id,
                json.dumps(result),
                success=success
            )
            
            success_indicator = "âœ…" if result.get('success') else "âŒ"
            self.logger.info(f"{success_indicator} Task {task_id} completed")
        except sqlite3.Error as e:
            self.logger.error(f"Database error while updating task result: {e}")
    
    def save_deliverable_files(self, task_id: str, task_result: Dict[str, Any], prompt: str, task_type: str):
        """è·äººã®ä½œæ¥­æˆæœã‚’å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆè¦ªæ–¹ã®è² æ‹…è»½æ¸›ï¼‰"""
        self.logger.info(f"ğŸ“ save_deliverable_files called for task {task_id}")
        try:
            # æˆæœç‰©ã‚’è§£æã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
            output = task_result.get('output', '')
            self.logger.info(f"ğŸ“„ Output length: {len(output)} characters")
            if not output:
                self.logger.warning(f"âš ï¸ No output content for task {task_id}, skipping file save")
                return
                
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç‰¹å®š
            project_root = self.detect_project_root(prompt)
            if not project_root:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹é…ä¸‹ã«ä¿å­˜
                project_root = os.getcwd()
                
            self.logger.info(f"ğŸ’¾ Saving deliverables for {task_id} to project: {project_root}")
            
            # ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®å ´åˆã¯å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            if self.is_code_generation_task(prompt, output):
                self.save_code_files(task_id, output, project_root, prompt)
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆã®å ´åˆ
            elif self.is_documentation_task(prompt, output):
                self.save_documentation_files(task_id, output, project_root)
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã®å ´åˆ
            elif self.is_config_task(prompt, output):
                self.save_config_files(task_id, output, project_root)
                
        except Exception as e:
            self.logger.error(f"Failed to save deliverable files: {e}")
    
    def detect_project_root(self, prompt: str) -> Optional[str]:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’æ¨å®š"""
        import re
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æŒ‡å®šã‚’æ¤œç´¢
        path_patterns = [
            r'é…ç½®å…ˆ[ï¼š:ï¼š]\s*([^\n\r]+)',
            r'ä¿å­˜å…ˆ[ï¼š:ï¼š]\s*([^\n\r]+)',
            r'ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®[ï¼š:ï¼š]\s*([^\n\r]+)',
            r'ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª[ï¼š:ï¼š]\s*([^\n\r]+)',
            r'ãƒ•ã‚©ãƒ«ãƒ€[ï¼š:ï¼š]\s*([^\n\r]+)'
        ]
        
        for pattern in path_patterns:
            match = re.search(pattern, prompt, re.IGNORECASE)
            if match:
                path = match.group(1).strip()
                # ãƒ‘ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå¼•ç”¨ç¬¦ã‚„ä½™åˆ†ãªæ–‡å­—ã‚’å‰Šé™¤ï¼‰
                path = path.strip('"\'`')
                
                # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
                if not os.path.isabs(path):
                    # ã¾ãšãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã¨ã—ã¦è§£é‡ˆ
                    project_root = '/home/hama/project/koubou-system'
                    full_path = os.path.join(project_root, path)
                    
                    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                    if os.path.exists(full_path):
                        return full_path if os.path.isdir(full_path) else os.path.dirname(full_path)
                    
                    # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                    parent_dir = os.path.dirname(full_path)
                    if os.path.exists(parent_dir) and os.path.isdir(parent_dir):
                        return parent_dir
                    
                    # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã¨ã—ã¦è©¦ã™
                    current_path = os.path.join(os.getcwd(), path)
                    if os.path.exists(current_path):
                        return current_path if os.path.isdir(current_path) else os.path.dirname(current_path)
                else:
                    # çµ¶å¯¾ãƒ‘ã‚¹ã®å ´åˆ
                    if os.path.exists(path):
                        return path if os.path.isdir(path) else os.path.dirname(path)
                    # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                    parent_dir = os.path.dirname(path)
                    if os.path.exists(parent_dir) and os.path.isdir(parent_dir):
                        return parent_dir
                    
        return None
    
    def is_code_generation_task(self, prompt: str, output: str) -> bool:
        """ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚¿ã‚¹ã‚¯ã‹ã©ã†ã‹åˆ¤å®š"""
        code_keywords = ['å®Ÿè£…', 'ã‚³ãƒ¼ãƒ‰', 'API', 'ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ', 'function', 'class', 'def ', 'import ', '```python', '```javascript', '```html']
        return any(keyword in prompt.lower() or keyword in output.lower() for keyword in code_keywords)
    
    def is_documentation_task(self, prompt: str, output: str) -> bool:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã‹ã©ã†ã‹åˆ¤å®š"""
        doc_keywords = ['README', 'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ', 'ãƒãƒ‹ãƒ¥ã‚¢ãƒ«', 'ã‚¬ã‚¤ãƒ‰', 'èª¬æ˜æ›¸', 'APIä»•æ§˜']
        return any(keyword in prompt for keyword in doc_keywords)
    
    def is_config_task(self, prompt: str, output: str) -> bool:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã‚¿ã‚¹ã‚¯ã‹ã©ã†ã‹åˆ¤å®š"""
        config_keywords = ['requirements.txt', 'package.json', '.env', 'config', 'è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«']
        return any(keyword in prompt for keyword in config_keywords)
    
    def save_code_files(self, task_id: str, output: str, project_root: str, prompt: str = ""):
        """ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆæ”¹è‰¯ç‰ˆ: Webé–‹ç™ºãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œï¼‰"""
        import re
        from datetime import datetime
        
        self.logger.info(f"ğŸ” Analyzing output for code files...")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã®æŒ‡å®šã‚’æ¤œç´¢
        files_to_create = {}
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ•ã‚¡ã‚¤ãƒ«åã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆ
        # ä¾‹: /* ====================== index.html ====================== */
        section_pattern = r'/\*\s*=+\s*([^\s=]+\.\w+)\s*=+\s*\*/'
        sections = re.split(section_pattern, output)
        
        if len(sections) > 1:
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å½¢å¼ã§åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹
            for i in range(1, len(sections), 2):
                if i+1 < len(sections):
                    filename = sections[i].strip()
                    content = sections[i+1].strip()
                    
                    # ã‚³ãƒ¡ãƒ³ãƒˆã‚„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®è£…é£¾ã‚’å‰Šé™¤
                    content = re.sub(r'^```\w*\n?', '', content)
                    content = re.sub(r'\n?```$', '', content)
                    
                    files_to_create[filename] = content
                    self.logger.info(f"ğŸ“„ Found file section: {filename}")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã«ãƒ•ã‚¡ã‚¤ãƒ«åãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if not files_to_create:
            code_blocks = re.findall(r'```(\w+)?\n(.*?)```', output, re.DOTALL)
            
            # ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŒã¤ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
            for block_content in code_blocks:
                lang = block_content[0] if isinstance(block_content, tuple) else None
                code = block_content[1] if isinstance(block_content, tuple) else block_content
                
                if not code or not isinstance(code, str):
                    continue
                    
                # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ¨å®š
                if '<!DOCTYPE html>' in code or '<html' in code:
                    files_to_create['index.html'] = code.strip()
                elif 'body' in code and '{' in code and 'margin' in code:
                    files_to_create['style.css'] = code.strip()
                elif ('function' in code or 'const' in code) and 'document' in code:
                    files_to_create['script.js'] = code.strip()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: é€šå¸¸ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰æ¨å®š
        if not files_to_create:
            code_blocks = re.findall(r'```(\w+)?\n(.*?)```', output, re.DOTALL)
            
            for i, (lang, code) in enumerate(code_blocks):
                if not code.strip():
                    continue
                    
                # è¨€èªã«å¿œã˜ãŸæ‹¡å¼µå­ã‚’æ±ºå®š
                ext_map = {
                    'python': '.py', 'javascript': '.js', 'html': '.html', 
                    'css': '.css', 'json': '.json', 'bash': '.sh',
                    'sql': '.sql', 'yaml': '.yml', 'jsx': '.jsx', 
                    'typescript': '.ts', 'tsx': '.tsx'
                }
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¨å®š
                if lang and lang.lower() in ext_map:
                    ext = ext_map[lang.lower()]
                    if ext == '.html':
                        filename = 'index.html'
                    elif ext == '.css':
                        filename = 'style.css'
                    elif ext in ['.js', '.jsx']:
                        filename = 'script.js'
                    elif ext == '.py':
                        filename = 'main.py'
                    else:
                        filename = f"file_{i}{ext}"
                else:
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ¨å®š
                    if '<!DOCTYPE html>' in code or '<html' in code:
                        filename = 'index.html'
                    elif 'body' in code and '{' in code and '}' in code:
                        filename = 'style.css'
                    elif 'function' in code or 'const' in code or 'let' in code:
                        filename = 'script.js'
                    else:
                        filename = f"output_{i}.txt"
                
                files_to_create[filename] = code.strip()
                self.logger.info(f"ğŸ“„ Found code block: {filename}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if files_to_create:
            # ã‚¿ã‚¹ã‚¯IDã‚’å«ã‚€ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            task_dir = os.path.join(project_root, f"task_{task_id}")
            os.makedirs(task_dir, exist_ok=True)
            
            for filename, content in files_to_create.items():
                filepath = os.path.join(task_dir, filename)
                
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
                os.makedirs(os.path.dirname(filepath), exist_ok=True)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(content)
                    
                self.logger.info(f"âœ… Saved: {filepath}")
            
            # ä½¿ã„æ–¹ã®èª¬æ˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç”Ÿæˆ
            readme_content = f"""# ã‚¿ã‚¹ã‚¯æˆæœç‰©: {task_id}

## ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
{chr(10).join(f"- {filename}" for filename in files_to_create.keys())}

## ä½¿ç”¨æ–¹æ³•
1. ãƒ–ãƒ©ã‚¦ã‚¶ã§ `index.html` ã‚’é–‹ã
2. ã¾ãŸã¯ã€HTTPã‚µãƒ¼ãƒãƒ¼ã§æä¾›:
   ```bash
   python -m http.server 8000
   # http://localhost:8000 ã«ã‚¢ã‚¯ã‚»ã‚¹
   ```

## ã‚¿ã‚¹ã‚¯è©³ç´°
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
            readme_path = os.path.join(task_dir, "README.md")
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(readme_content)
            self.logger.info(f"ğŸ“š Saved README: {readme_path}")
        else:
            self.logger.warning(f"âš ï¸ No separate code files detected, analyzing as single output")
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            filepath = os.path.join(project_root, f"output_{task_id}.txt")
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(output)
            self.logger.info(f"ğŸ“„ Saved as text: {filepath}")
    
    def save_documentation_files(self, task_id: str, output: str, project_root: str):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
        filename = "README.md"
        if "API" in output:
            filename = "API_DOCS.md"
        elif "é–‹ç™ºè€…" in output or "developer" in output.lower():
            filename = "DEVELOPER_GUIDE.md"
            
        filepath = os.path.join(project_root, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(output)
            
        self.logger.info(f"ğŸ“– Saved documentation: {filepath}")
    
    def save_config_files(self, task_id: str, output: str, project_root: str):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
        if 'requirements.txt' in output or ('pip install' in output and any(pkg in output for pkg in ['fastapi', 'flask', 'django'])):
            filepath = os.path.join(project_root, 'requirements.txt')
            # requirementså½¢å¼ã§æŠ½å‡º
            lines = []
            for line in output.split('\n'):
                if '==' in line or '>=' in line or line.strip().endswith('.txt'):
                    lines.append(line.strip())
            if lines:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(lines))
                self.logger.info(f"âš™ï¸ Saved config: {filepath}")

    def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†"""
        try:
            db.update_worker_status(self.worker_id, 'offline', None)
            self.logger.info(f"Worker {self.worker_id} cleanup completed")
        except:
            pass
    
    def cleanup_handler(self, signum, frame):
        """ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
        self.cleanup()
        sys.exit(0)

    def run(self):
        """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        self.logger.info(f"ğŸš€ Gemini worker {self.worker_id} started (model: {self.model})")
        
        try:
            heartbeat_counter = 0
            while True:
                # Pool Managerã‹ã‚‰ã®ã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦é€šçŸ¥ã‚’ãƒã‚§ãƒƒã‚¯
                assigned_task_id = self.check_for_task_notifications()
                
                if assigned_task_id:
                    # æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’å–å¾—ã—ã¦å‡¦ç†
                    task = self.get_assigned_task(assigned_task_id)
                    if task:
                        self.logger.info(f"ğŸ”¥ Processing assigned task: {assigned_task_id}")
                        result = self.process_task(task)
                        self.update_task_result(task['task_id'], result)
                    else:
                        self.logger.warning(f"Could not retrieve assigned task: {assigned_task_id}")
                else:
                    # é€šçŸ¥ãŒãªã„å ´åˆã¯å°‘ã—å¾…æ©Ÿ
                    time.sleep(1)
                    # 10ç§’ã”ã¨ã«ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚’é€ä¿¡
                    heartbeat_counter += 1
                    if heartbeat_counter >= 10:
                        db.update_worker_status(self.worker_id, 'idle', None)
                        heartbeat_counter = 0
        except KeyboardInterrupt:
            self.logger.info("ğŸ›‘ Worker interrupted by user")
        except Exception as e:
            self.logger.error(f"ğŸ’¥ Unexpected error in worker main loop: {e}")
        finally:
            # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã«è¨­å®š
            db.update_worker_status(self.worker_id, 'offline', None)
            self.logger.info(f"ğŸ‘‹ Gemini worker {self.worker_id} stopped")

if __name__ == "__main__":
    worker = GeminiLocalWorker()
    worker.run()